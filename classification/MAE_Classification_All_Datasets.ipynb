{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1274,"status":"ok","timestamp":1606911946365,"user":{"displayName":"Ishita Mediratta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcSCnDdunLdgYslerqgLs_0bK8yebqL3eZbUIL-TA=s64","userId":"03053038547713479079"},"user_tz":-330},"id":"tPI-xQaafp7D","outputId":"102b13de-9f65-443f-a784-fb95ab77b2fb"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3835,"status":"ok","timestamp":1606911948935,"user":{"displayName":"Ishita Mediratta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcSCnDdunLdgYslerqgLs_0bK8yebqL3eZbUIL-TA=s64","userId":"03053038547713479079"},"user_tz":-330},"id":"ZSqkEn4sgARk"},"outputs":[],"source":["# Commented out IPython magic to ensure Python compatibility.\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","from __future__ import print_function\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score\n","# %matplotlib inline\n","\n","from keras.engine.base_layer import Layer\n","from keras.layers import Activation, Dense, ReLU\n","from keras import backend as K\n","from sklearn.model_selection import train_test_split\n","from keras.datasets import mnist\n","from keras.optimizers import SGD\n","from keras.utils import np_utils\n","\n","import keras\n","from keras.models import Sequential\n","from keras.layers.core import Flatten\n","from keras.layers import Dropout\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers.normalization import BatchNormalization\n","from keras import models\n","from keras import layers\n","import numpy as np\n","import time\n","from tqdm import tqdm\n","import pandas as pd\n","import os\n","import gc\n","import math\n","from matplotlib import pyplot\n","\n","home_dir = os.path.join('/content/drive/MyDrive/liparelu_classification/multiclass')\n","log_dir = os.path.join('/content/drive/MyDrive/liparelu_classification/logs')\n","\n","datasets_to_use = os.listdir(home_dir)\n","datasets_to_use.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1263007,"status":"ok","timestamp":1606913208113,"user":{"displayName":"Ishita Mediratta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcSCnDdunLdgYslerqgLs_0bK8yebqL3eZbUIL-TA=s64","userId":"03053038547713479079"},"user_tz":-330},"id":"U57RRS8ZfmFZ","outputId":"7900bcc7-a9d0-4015-c912-e4e8b0db5851"},"outputs":[],"source":["\n","for datasets in datasets_to_use:\n","\tif not ('.csv' in datasets):\n","\t\tcontinue\n","\n","\tif 'Indian Liver' in datasets:\n","\t\tcontinue\n","\n","\tdset = datasets[0:-4]\n","\tprint(dset)\n","\tK.clear_session()\n","\t\n","\n","\tdf = pd.read_csv(os.path.join(home_dir,dset+'.csv'), header=None)\n","\tdf.head()\n","\n","\t# Preprocessing training data\n","\t_, c = df.shape\n","\ty = np.asarray(df[c-1])\n","\tx = np.asarray(df.drop([c-1], axis=1))\n","\n","\tx.shape, y.shape\n","\n","\tfrom sklearn.model_selection import train_test_split\n","\tx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","\tx_train.shape, x_test.shape, y_train.shape, y_test.shape\n","\n","\tfrom keras.utils import to_categorical\n","\n","\t# y_train_cat = to_categorical(y_train)\n","\t# y_test_cat  = to_categorical(y_test)\n","\n","\tfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n","\n","\timport tensorflow as tf\n","\n","\tclass ApproxRelu(Layer):\n","\t\t\n","\t\tdef __init__(self, k=0.5, n=1.15, **kwargs):\n","\t\t\tsuper(ApproxRelu, self).__init__(**kwargs)\n","\t\t\tself.supports_masking = True\n","\t\t\tself.k = K.cast_to_floatx(k)\n","\t\t\tself.n = K.cast_to_floatx(n)\n","\n","\t\tdef call(self, inputs):\n","\t\t\torig = inputs\n","\t\t\tinputs = tf.where(orig <= 0.0, tf.zeros_like(inputs), inputs)\n","\t\t\tinputs = tf.where(tf.greater(orig, 0.0), self.k*tf.pow(inputs, self.n), inputs)\n","\t\t\treturn  inputs\n","\n","\t\tdef get_config(self):\n","\t\t\tconfig = {'k': float(self.k), 'n': float(self.n)}\n","\t\t\tbase_config = super(ApproxRelu, self).get_config()\n","\t\t\treturn dict(list(base_config.items()) + list(config.items()))\n","\n","\t\tdef compute_output_shape(self, input_shape):\n","\t\t\treturn input_shape\n","\n","\tclass AbstractModel:\n","\t\t\"\"\"\n","\t\tThe base class for all Model classes\n","\t\t\"\"\"\n","\n","\t\tdef __init__(self, n_classes = 2, activation='arelu', classification=True, optimizer='sgd',\n","\t\t\t\t\t bs = 8, epochs= 100, loss = 'categorical_crossentropy', adaptive = True,\n","\t\t\t\t\t convergence = True, num_hiddden_layers=1, num_hidden_neurons=10, x_train=None, x_test=None, y_train=None, y_test=None):\n","\n","\n","\t\t\tif n_classes < 2:\n","\t\t\t\traise ValueError('n_classes must be at least 2.')\n","\t\t\tif optimizer == 'sgd':\n","\t\t\t\tself.optimizer = SGD(lr=1e-1, decay=1e-4)\n","\t\t\telse:\n","\t\t\t\traise NotImplementedError('Only SGD optimizer is implemented!')\n","\t\t\tif bs < 1 or not isinstance(bs, int):\n","\t\t\t\traise ValueError('Improper batch size')\n","\t\t\tif epochs < 1:\n","\t\t\t\traise ValueError('Invalid number of epochs')\n","\t\t\tif not (loss in [\"categorical_crossentropy\", \"mae\", \"quadratic\"]):\n","\t\t\t\traise ValueError('Invalid loss function specified')\n","\n","\t\t\tself.activation = activation\n","\t\t\tself.classification = classification\n","\t\t\tself.bs = bs\n","\t\t\n","\t\t\tself.n_classes = n_classes\n","\t\t\tself.epochs = epochs\n","\t\t \n","\t\t\tself.adaptive = adaptive\n","\t\t\tself.convergence = convergence\n","\t\t\tself.num_hiddden_layers = num_hiddden_layers\n","\t\t\tself.num_hidden_neurons = num_hidden_neurons\n","\n","\t\t\tself.lr_history = []\n","\t\t\tself.model_history = None\n","\t\t\tself.K_z = []\n","\n","\t\t\tself.model = None\n","\t\t\tself.callbacks = None\n","\n","\t\t\tif self.convergence or self.adaptive:\n","\t\t\t\tself.callbacks = []\n","\n","\t\t\tif self.convergence:\n","\t\t\t\tself.epochs = 20000000 #very large value\n","\n","\n","\t\t\tself.loss = loss\n","\n","\t\t\tif loss==\"categorical_crossentropy\" and self.n_classes==2:\n","\t\t\t\tself.loss = \"binary_crossentropy\"\n","\n","\t\t\tself.metrics = None\n","\t\t\tif loss==\"categorical_crossentropy\":\n","\t\t\t\tself.metrics = [\"accuracy\"]\n","\t\t\telse:\n","\t\t\t\tself.metrics = [loss]\n","\n","\t\t\tself.x_train = x_train\n","\t\t\tself.inp_shape = x_train.shape[1]\n","\t\t\tself.x_test = x_test\n","\t\t\tself.y_train = y_train\n","\t\t\tself.y_test = y_test\n","\t\t\t\n","\t\t\tself.K_a = None\n","\n","\t\t\tself._scale_data()\n","\n","\t\tdef _scale_data(self):\n","\t\t\tprint(\"Scaling...\")\n","\t\t\tfrom sklearn.preprocessing import MinMaxScaler\n","\t\t\tscaler_train = MinMaxScaler()\n","\t\t\tself.x_train = scaler_train.fit_transform(self.x_train)\n","\t\t\tscaler_test = MinMaxScaler()\n","\t\t\tself.x_test = scaler_test.fit_transform(self.x_test)\n","\n","\n","\t\tdef _get_model(self):\n","\t\t\t\"\"\"\n","\t\t\tGet a model instance.\n","\t\t\t:return: Keras Model instance\n","\t\t\t\"\"\"\n","\t\t\tnetwork = models.Sequential()\n","\t\t\tnetwork.add(layers.Dense(self.num_hidden_neurons, input_shape=(self.inp_shape, )))\n","\t\t\tnetwork.add(ApproxRelu())\n","\t\t\tfor i in range(self.num_hiddden_layers-1):\n","\t\t\t\tnetwork.add(layers.Dense(self.num_hidden_neurons))\n","\t\t\t\tnetwork.add(ApproxRelu())\n","\t\t\tif self.loss=='categorical_crossentropy':\n","\t\t\t\tnetwork.add(layers.Dense(self.n_classes, activation='softmax'))\n","\t\t\telif self.loss=='binary_crossentropy':\n","\t\t\t\tnetwork.add(Dense(1, activation='sigmoid'))\n","\t\t\telse:\n","\t\t\t\t# for mae and mse we assume Arelu in the output neuron\n","\t\t\t\tnetwork.add(Dense(1, activation='linear'))\n","\t\t\t\t# network.add(layers.Dense(1))\n","\t\t\t\t# network.add(ApproxRelu())\n","\t\t\treturn network\n","\n","\t\tdef _lr_schedule(self, epoch: int):\n","\t\t\t\"\"\"\n","\t\t\tGet the learning rate for a given epoch. Note that this uses the LipschitzLR policy, so the epoch\n","\t\t\tnumber doesn't actually matter.\n","\t\t\t:param epoch: int. Epoch number\n","\t\t\t:return: learning rate\n","\t\t\t\"\"\"\n","\n","\t\t\t# if self.task == 'regression':\n","\t\t\t#     # TODO: Implement this with LipschitzLR\n","\t\t\t#     return 0.1\n","\n","\t\t\t\n","\t\t\tif self.x_train is None:\n","\t\t\t\traise ValueError('x_train is None')\n","\n","\t\t\tif self.loss == 'categorical_crossentropy' or self.loss == 'binary_crossentropy':\n","\n","\t\t\t\tpenultimate_activ_func = K.function([self.model.layers[0].input], [self.model.layers[-2].output])\n","\n","\t\t\t\tK_max=-1.0\n","\t\t\t\tfor i in range((len(self.x_train) - 1) // self.bs + 1):\n","\t\t\t\t\tstart_i = i * self.bs\n","\t\t\t\t\tend_i = start_i + self.bs\n","\t\t\t\t\txb = self.x_train[start_i:end_i]\n","\n","\t\t\t\t\tactiv = np.linalg.norm(penultimate_activ_func([xb]))\n","\t\t\t\t\tKz = activ\n","\t\t\t\t\t\n","\t\t\t\t\tL=Kz\n","\t\t\t\t\tif(L>K_max):\n","\t\t\t\t\t\tK_max=L\n","\n","\t\t\t\tK_ = (((self.n_classes - 1) * K_max) / (self.n_classes * self.bs))\n","\t\t\t\t\n","\t\t\t\tif K_ != 0:\n","\t\t\t\t\t\n","\t\t\t\t\tlr = float(1 / K_)\n","\t\t\t\t\tlr = lr * 0.1\n","\t\t\t\t\t\n","\t\t\t\telse:\n","\t\t\t\t\tlr = 0.1\n","\t\t\t\t\t\n","\t\t\t\tif lr > 20.:\n","\t\t\t\t\tlr = 0.1\n","\t\t\t\t\n","\t\t\t\tself.lr_history.append(lr)\n","\t\t\t\tself.K_z.append(K_max)\n","\t\t\t\t\n","\t\t\t\treturn lr\n","\t\t\t\t\n","\n","\t\t\telif self.loss == 'mae':\n","\t\t\t\t\n","\t\t\t\tpenultimate_activ_func = K.function([self.model.layers[0].input], [self.model.layers[-2].output])\n","\t\t\t\t#final_logit_func = K.function([self.model.layers[0].input], [self.model.layers[-2].output])\n","\n","\t\t\t\tK_max=-1.0\n","\t\t\t\tfor i in range((len(self.x_train) - 1) // self.bs + 1):\n","\t\t\t\t\tstart_i = i * self.bs\n","\t\t\t\t\tend_i = start_i + self.bs\n","\t\t\t\t\txb = self.x_train[start_i:end_i]\n","\n","\t\t\t\t\tactiv = np.linalg.norm(penultimate_activ_func([xb]))\n","\t\t\t\t\tKz = activ\n","\t\t\t\t\t\n","\t\t\t\t\tL=Kz\n","\t\t\t\t\tif(L>K_max):\n","\t\t\t\t\t\tK_max=L\n","\t\t\t\t\t\t\n","\t\t\t\t# Zj = 0.\n","\t\t\t\t# for i in range((len(self.x_train) - 1) // self.bs + 1):\n","\t\t\t\t# \tstart_i = i * self.bs\n","\t\t\t\t# \tend_i = start_i + self.bs\n","\t\t\t\t# \txb = self.x_train[start_i:end_i]\n","\n","\t\t\t\t# \tactiv = np.max(final_logit_func([xb]))\n","\t\t\t\t# \tif activ > Zj:\n","\t\t\t\t# \t\tZj = activ\n","\t\t\t\t\t\t\n","\t\t\t\tK_ = K_max/self.bs #((K_max * (Zj**0.1)) / (2*self.bs))\n","\t\t\t\t\n","\t\t\t\tif K_ != 0:\n","\t\t\t\t\t\n","\t\t\t\t\tlr = float(1 / K_)\n","\t\t\t\t\tlr = lr * 0.01\n","\t\t\t\t\t\n","\t\t\t\telse:\n","\t\t\t\t\tlr = 0.1\n","\t\t\t\t\t\n","\t\t\t\tif lr > 20.:\n","\t\t\t\t\tlr = 0.1\n","\t\t\t\t\n","\t\t\t\t\n","\t\t\t\t\n","\t\t\t\tself.lr_history.append(lr)\n","\t\t\t\tself.K_z.append(K_max)\n","\t\t\t\t\n","\t\t\t\t\n","\t\t\t\treturn lr\n","\n","\t\t\t\n","\t\t\telif self.loss == 'mse':\n","\t\t\t\treturn 0.1\n","\t\t\t\t\n","\t#             penultimate_activ_func = K.function([self.model.layers[0].input], [self.model.layers[-2].output])\n","\t#             penultimate_activ_func = K.function([self.model.layers[0].input], [self.model.layers[-2].output])\n","\n","\t#             Kz = 0.\n","\t#             for i in range((len(self.x_train) - 1) // self.bs + 1):\n","\t#                 start_i = i * self.bs\n","\t#                 end_i = start_i + self.bs\n","\t#                 xb = self.x_train[start_i:end_i]\n","\n","\t#                 activ = np.linalg.norm(penultimate_activ_func([xb]))\n","\t#                 if activ > Kz:\n","\t#                     Kz = activ\n","\t\t\t\t\t\t\n","\t#             maxwt = np.max(model.layers[-2].get_weights()[0])\n","\t\t\t\t\t\t\n","\t#             K_ = (Kz * (maxwt**1.1)) / (2*self.bs)\n","\t\t\t\t\n","\n","\n","\n","\t\t\treturn 0.1\n","\t\t\t\n","\t\t\t\n","\n","\t\tdef fit(self):\n","\t\t\t\"\"\"\n","\t\t\tFit to data\n","\t\t\t:return: None\n","\t\t\t\"\"\"\n","\n","\t\t\tif self.x_train is None or self.x_test is None or \\\n","\t\t\t\t self.y_train is None or self.y_test is None:\n","\t\t\t\traise ValueError('Data is None')\n","\n","\t\t\tself.model = self._get_model()\n","\n","\t\t\tif self.convergence:\n","\t\t\t\tearly_stopper = EarlyStopping(monitor='val_loss', patience=10, mode='min', min_delta=0.0001)\n","\t\t\t\tself.callbacks.append(early_stopper)\n","\n","\t\t\tif self.adaptive:\n","\t\t\t\tlr_scheduler = LearningRateScheduler(self._lr_schedule)\n","\t\t\t\tself.callbacks.append(lr_scheduler)\n","\n","\t\t\tself.model.compile(self.optimizer, loss=self.loss, metrics=self.metrics)\n","\t\t\tself.model_history = self.model.fit(self.x_train, self.y_train, epochs=self.epochs,\n","\t\t\t\t\t\t\t batch_size=self.bs, callbacks=self.callbacks, validation_data=(self.x_test, self.y_test), verbose=0)\n","\n","\t\tdef predict(self, x: np.ndarray):\n","\t\t\t\"\"\"\n","\t\t\tPredict on new data\n","\t\t\t:param x: array-like\n","\t\t\t:return: predictions:  array-like\n","\t\t\t\"\"\"\n","\t\t\treturn self.model.predict(x)\n","\t \n","\t\tdef get_acc(self):\n","\t\t\ty_pred_test = self.model.predict(self.x_test)\n","\t\t\ty_pred_train = self.model.predict(self.x_train)\n","\n","\t\t\ty_final_train = []\n","\t\t\tfor pred in y_pred_train:\n","\t\t\t\tval = math.floor(pred)\n","\t\t\t\tval = max(0, val)\n","\t\t\t\tval = min(val, np.max(self.y_train))\n","\t\t\t\ty_final_train.append(val)\n","\t\t\t\n","\n","\t\t\ty_final_test = []\n","\t\t\tfor pred in y_pred_test:\n","\t\t\t\tval = math.floor(pred)\n","\t\t\t\tval = max(0, val)\n","\t\t\t\tval = min(val, np.max(self.y_test))\n","\t\t\t\ty_final_test.append(val)\n","\n","\t\t\treturn accuracy_score(self.y_test, y_final_test), accuracy_score(self.y_train, y_final_train)\n","\n","\t\tdef score(self):\n","\t\t\t\"\"\"\n","\t\t\tReturns model performance on test set\n","\t\t\t:return:\n","\t\t\t\"\"\"\n","\n","\t\t\tif self.x_test is None or self.y_test is None:\n","\t\t\t\traise ValueError('Test data is None')\n","\n","\t\t\ttrain_loss, _ = self.model.evaluate(self.x_train, self.y_train)\n","\t\t\ttest_loss, _ = self.model.evaluate(self.x_test, self.y_test)\n","\t\t\t\n","\t\t\ttest_metric, train_metric = self.get_acc()\n","\t\t\t\n","\t\t\treturn train_loss, train_metric, test_loss, test_metric\n","\n","\t\tdef plot_lr(self):\n","\t\t\t\"\"\"\n","\t\t\tPlots learning rate history\n","\t\t\t:return: None\n","\t\t\t\"\"\"\n","\t\t\tplt.style.use('ggplot')\n","\t\t\tplt.plot(range(self.epochs), self.lr_history)\n","\n","\t\tdef get_history(self):\n","\t\t\t\"\"\"\n","\t\t\treturns model history\n","\t\t\t:return:\n","\t\t\t\"\"\"\n","\n","\t\t\treturn self.model_history.history[\"loss\"]\n","\n","\tarelu_cc = None\n","\tcolumns = ['Experiment','Train_Accuracy','Test_Accuracy','Train_loss','Test_loss']\n","\n","\thidden_layers = [1]\n","\thidden_width = [4, 5, 8, 10,20,50]\n","\tloss_metric = [['mae','mae']]\n","\n","\thistory_cce = []\n","\thistory_mae = []\n","\tstored_lr_hist = []\n","\n","\n","\tfor lm in loss_metric:\n","\t\tarelu_scores = []\n","\t\tfor layer in hidden_layers:\n","\t\t\tfor width in hidden_width:\n","\t\t\t\tfor lr in [\"lalr\", \"base\"]:\n","\t\t\t\t\tprint('arelu',layer,width,lr)\n","\t\t\t\t\tK.clear_session()\n","\t\t\t\t\tif lm[0]=='mse' and lr=='lalr':\n","\t\t\t\t\t\tcontinue\n","\t\t\t\t\tif lm[0]=='categorical_crossentropy':\n","\t\t\t\t\t\tclassification = True\n","\t\t\t\t\t# train_y, test_y = y_train_cat, y_test_cat\n","\t\t\t\t\telse:\n","\t\t\t\t\t\tclassification = False\n","\t\t\t\t\t# train_y, test_y = y_train, y_test\n","\n","\t\t\t\t\tif lr=='lalr':\n","\t\t\t\t\t\tadaptive=True\n","\t\t\t\t\telse:\n","\t\t\t\t\t\tadaptive=False\n","\n","\t\t\t\t\tshape1, _ = x_train.shape\n","\t\t\t\t\tif shape1<500:\n","\t\t\t\t\t\tbs=2\n","\t\t\t\t\telif shape1 < 1000:\n","\t\t\t\t\t\tbs=4\n","\t\t\t\t\telif shape1 < 5000:\n","\t\t\t\t\t\tbs = 16\n","\t\t\t\t\telse:\n","\t\t\t\t\t\tbs = 32\n","\t\t\t\t\tnetwork = AbstractModel(bs=bs, loss=lm[0], adaptive=adaptive, num_hiddden_layers=layer, num_hidden_neurons=width, classification=classification, x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n","\t\t\t\t\tnetwork.fit()\n","\t\t\t\t\ttrain_loss, train_metric, test_loss, test_metric = network.score()\n","\t\t\t\t\thistory = network.get_history()\n","\t\t\t\t\texp_name = str(layer)+'_'+str(width)+'_'+lr\n","\t\t\t\t\tarelu_scores.append([exp_name, train_metric, test_metric, train_loss, test_loss])\n","\t\t\t\t\t\n","\t\t\t\t\tif lm[0]=='categorical_crossentropy':\n","\t\t\t\t\t\thistory_cce.append([exp_name, history])\n","\t\t\t\t\telif lm[0]=='mse':\n","\t\t\t\t\t\thistory_mse.append([exp_name, history])\n","\t\t\t\t\telse:\n","\t\t\t\t\t\thistory_mae.append([exp_name, history])\n","\t\t\t\t\tif lr=='lalr':\n","\t\t\t\t\t\tstored_lr_hist.append([exp_name, network.lr_history])\n","\n","\tif lm[0]=='categorical_crossentropy':\n","\t\tarelu_cc = pd.DataFrame(arelu_scores[:],columns=columns)\n","\telif lm[0]=='mse':\n","\t\tarelu_mse = pd.DataFrame(arelu_scores[:],columns=columns)\n","\telse:\n","\t\tarelu_mae = pd.DataFrame(arelu_scores[:],columns=columns)\n","\n","\tpyplot.title('LR')\n","\tpyplot.xlabel('Epoch')\n","\tpyplot.ylabel('LR')\n","\tfor i,lr in enumerate(stored_lr_hist):\n","\t\tpyplot.plot(lr[1], label=lr[0])\n","\tpyplot.legend()\n","\tpyplot.savefig(os.path.join(log_dir,dset+'_mae_lr.png'))\n","\tpyplot.show()\n","\n","\tepochs_taken = [len(i[1]) for i in history_mae]\n","\tarelu_mae['epochs_conv'] = epochs_taken\n","\tarelu_mae\n","\n","\tarelu_mae.to_csv(os.path.join(log_dir,dset+'_mae.csv'), index=False)\n","\n","\tpyplot.title('Learning Curves')\n","\tpyplot.xlabel('Epoch')\n","\tpyplot.ylabel('MAE Loss')\n","\tfor history in history_mae:\n","\t\tpyplot.plot(history[1], label=history[0])\n","\tpyplot.legend()\n","\tpyplot.savefig(os.path.join(log_dir,dset+'_mae_history.png'))\n","\tpyplot.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1263001,"status":"ok","timestamp":1606913208115,"user":{"displayName":"Ishita Mediratta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcSCnDdunLdgYslerqgLs_0bK8yebqL3eZbUIL-TA=s64","userId":"03053038547713479079"},"user_tz":-330},"id":"FaqvSBjggDoJ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"MAE_Classification_All_Datasets.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
